{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68342df431724e04a180366320717a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_315b561881914b28adbcc51a11a558be",
              "IPY_MODEL_11de59bdb58e43258363b971012fb755",
              "IPY_MODEL_b84126cb8e0c432aacd1233ca6563220"
            ],
            "layout": "IPY_MODEL_f96c732d6d5949d7a0b9a67dce76cb92"
          }
        },
        "315b561881914b28adbcc51a11a558be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f9fc72b7aad404893fd33534f681299",
            "placeholder": "​",
            "style": "IPY_MODEL_fef66e84a856495aa5435569833429c4",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "11de59bdb58e43258363b971012fb755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_342f1d0886e240f8abecfa8d3e7a8751",
            "max": 381,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2148ce31394c4e14a2a5a7bf442cc207",
            "value": 381
          }
        },
        "b84126cb8e0c432aacd1233ca6563220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d72317c387794d33b1efc03dc8b93b96",
            "placeholder": "​",
            "style": "IPY_MODEL_b7eef5cfff8941efbee7a2a8eaaed7e1",
            "value": " 381/381 [00:02&lt;00:00, 190.49 examples/s]"
          }
        },
        "f96c732d6d5949d7a0b9a67dce76cb92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f9fc72b7aad404893fd33534f681299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef66e84a856495aa5435569833429c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "342f1d0886e240f8abecfa8d3e7a8751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2148ce31394c4e14a2a5a7bf442cc207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d72317c387794d33b1efc03dc8b93b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7eef5cfff8941efbee7a2a8eaaed7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MevERbQSXLXc"
      },
      "outputs": [],
      "source": [
        "! pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface-hub"
      ],
      "metadata": {
        "id": "Ra4kTg5tX8BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "8bFm_A-LZNto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer  # trainer for supervised fine-tuning (SFT)\n",
        "from unsloth import is_bfloat16_supported  # checks if the hardware supports bfloat16 operations\n",
        "\n",
        "from huggingface_hub import login\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset  # Lets you load fine-tuning datasets in huggingface\n",
        "\n",
        "\n",
        "hugging_face_token = os.environ[\"HF_TOKEN\"]\n",
        "login(hugging_face_token)"
      ],
      "metadata": {
        "id": "rIMd1kU0X96V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paramerts to load pre-trained model\n",
        "max_seq_length = 1024  # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\n",
        "dtype = None  # set to default\n",
        "load_in_4bit = True  # Enables 4-bit quantization - a memory saving optimization\n",
        "\n",
        "# Load the R1 model and tokenizer using unsloth - imported using FastLanguageModel\n",
        "model_og, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\",  # ref: https://huggingface.co/unsloth/DeepSeek-R1\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    token=hugging_face_token\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwwzS8UHX9zC",
        "outputId": "1155a0d4-b3ad-4064-db5e-25e64cb9a31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.1.8: Fast Qwen2 patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Given a question think and try to answer the question,\n",
        "Before answering, think when needed based in the question and create a step-by-step chain of thoughts.\n",
        "Do reverify your answers and reasonsing and correct any mistakes.\n",
        "\n",
        "### Instruction:\n",
        "Only think when required\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\""
      ],
      "metadata": {
        "id": "N57BUsvHX9wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is (7154 + -92) * 1936 / -1928 ?\"\n",
        "FastLanguageModel.for_inference(model_og)\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "# generate response using the model\n",
        "outputs = model_og.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    use_cache=True\n",
        ")\n",
        "# decode the generated output tokens into human-readable text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH8AHcB_X9t7",
        "outputId": "7d6822a4-5b51-486d-bf39-1c9414e32f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "To solve the expression \\((7154 + -92) \\times 1936 / -1928\\), we will follow the order of operations (PEMDAS/BODMAS):\n",
            "\n",
            "1. **Parentheses/Brackets**: First, evaluate the expression inside the parentheses.\n",
            "   \\[\n",
            "   7154 + (-92) = 7154 - 92 = 7062\n",
            "   \\]\n",
            "   \n",
            "2. **Multiplication and Division**: Next, perform multiplication and division from left to right.\n",
            "   \\[\n",
            "   7062 \\times 1936 = ?\n",
            "   \\]\n",
            "   Let's compute this step by step:\n",
            "   \\[\n",
            "   7062 \\times 1936 = 7062 \\times (1000 + 900 + 30 + 6) = 7062 \\times 1000 + 7062 \\times 900 + 7062 \\times 30 + 7062 \\times 6\n",
            "   \\]\n",
            "   \\[\n",
            "   = 7,062,000 + 6,355,800 + 211,860 + 42,372 = 13,079,032\n",
            "   \\]\n",
            "   Now, divide the result by \\(-1928\\):\n",
            "   \\[\n",
            "   13,079,032 \\div (-1928) = -6768\n",
            "   \\]\n",
            "   \n",
            "3. **Final Answer**: The result of the expression is \\(-6768\\).\n",
            "\n",
            "\\[\n",
            "\\boxed{-6768}\n",
            "\\]<｜end▁of▁sentence｜>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use supervised fine-tuning with training prompts and existing thinking\n",
        "train_prompt_style = \"\"\"Given a question think and try to answer the question,\n",
        "Before answering, think when needed based in the question and create a step-by-step chain of thoughts.\n",
        "Do reverify your answers and reasonsing and correct any mistakes.\n",
        "\n",
        "### Instruction:\n",
        "Only think when required\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"Question\"]\n",
        "    cots = examples[\"Complex_CoT\"]\n",
        "    outputs = examples[\"Response\"]\n",
        "    texts = []\n",
        "    for input, cot, output in zip(inputs, cots, outputs):\n",
        "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "MN2e_zEdX9qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# Load dataset from a local JSONL file\n",
        "dataset = load_dataset(\"json\", data_files=\"dummy_coldstart.jsonl\")\n",
        "\n",
        "# Access the dataset split (default is 'train' if no split is defined)\n",
        "print(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "dataset['train'][0]\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model_og,\n",
        "    r=8,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=2025,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZfPNxz9YP1G",
        "outputId": "f87ddace-e6a7-4831-c482-598b1570bf18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Question', 'Complex_CoT', 'Response'],\n",
            "        num_rows: 381\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "hiuF9yBVYPti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps=5,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513,
          "referenced_widgets": [
            "68342df431724e04a180366320717a62",
            "315b561881914b28adbcc51a11a558be",
            "11de59bdb58e43258363b971012fb755",
            "b84126cb8e0c432aacd1233ca6563220",
            "f96c732d6d5949d7a0b9a67dce76cb92",
            "3f9fc72b7aad404893fd33534f681299",
            "fef66e84a856495aa5435569833429c4",
            "342f1d0886e240f8abecfa8d3e7a8751",
            "2148ce31394c4e14a2a5a7bf442cc207",
            "d72317c387794d33b1efc03dc8b93b96",
            "b7eef5cfff8941efbee7a2a8eaaed7e1"
          ]
        },
        "id": "sNGM1rQTYPdg",
        "outputId": "c658a3da-38c2-4452-9d1a-0fc34d7298e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/381 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68342df431724e04a180366320717a62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 381 | Num Epochs = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 100\n",
            " \"-____-\"     Number of trainable parameters = 9,232,384\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 05:41, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.218600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.994100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.641100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.473200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.443000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.456700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.489500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.537600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is (7154 + -92) * 1936 / -1928 ?\"\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peZyXmJJcFnw",
        "outputId": "134518bf-7198-4ebc-b483-b5eb9011dec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Problem: What is (7154 + -92) * 1936 / -1928?  \n",
            "Reasoning:  \n",
            "1. Simplify the expression inside the parentheses: 7154 + (-92) = 7062  \n",
            "2. Multiply the result by 1936: 7062 * 1936 = 136,162,872  \n",
            "3. Divide the result by -1928: 136,162,872 / -1928 = -7062  \n",
            "\n",
            "Recheck the steps and correct any mistakes:\n",
            "1. The initial operation inside the parentheses is correct (7154 - 92 = 7062).  \n",
            "2. Multiplying 7062 by 1936 gives 136,162,872, which is accurate.  \n",
            "3. Finally, dividing this value by -1928 results in -7062, which is correct.\n",
            "</think>\n",
            "<think> \n",
            "Problem: What is (7154 + -92) * 1936 / -1928?  \n",
            "Reasoning:  \n",
            "1. Simplify the expression inside the parentheses: 7154 + (-92) = 7062  \n",
            "2. Multiply the result by 1936: 7062 * 1936 = 136,162,872  \n",
            "3. Divide the result by -1928: 136,162,872 / -1928 = -7062  \n",
            "\n",
            "Recheck the steps and correct any mistakes:\n",
            "1. The initial operation inside the parentheses is correct (7154 - 92 = 7062).  \n",
            "2. Multiplying 7062 by 1936 gives 136,162,872, which is accurate.  \n",
            "3. Finally, dividing this value by -1928 results in -7062, which is correct.\n",
            "</think>\n",
            "<answer>-7062</answer>\n",
            "<verifier_answer>-7062</verifier_answer><｜end▁of▁sentence｜>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_local = \"DeepSeek-R1-Test-Qwen-COT\"\n",
        "model.save_pretrained(new_model_local)\n",
        "tokenizer.save_pretrained(new_model_local)\n",
        "\n",
        "model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdEofMoecLsO",
        "outputId": "d086725f-ca9b-4521-aac3-1150306aaf44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 1.8G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.89 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:00<00:00, 53.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving DeepSeek-R1-Test-Qwen-COT/pytorch_model.bin...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### -------- PUSH TO HUB -------- ####\n",
        "# new_model_online = \"***/DeepSeek-R1-Test-COT\"\n",
        "# model.push_to_hub(new_model_online)\n",
        "# tokenizer.push_to_hub(new_model_online)\n",
        "\n",
        "# model.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")"
      ],
      "metadata": {
        "id": "oEUWyOsTcRO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zip -r DeepSeek-R1-Test-Qwen-COT.zip DeepSeek-R1-Test-Qwen-COT/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wng5WJrZhtPN",
        "outputId": "c20c8ffa-4bbd-4363-ef7a-443a5b17aaf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: DeepSeek-R1-Test-Qwen-COT/ (stored 0%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/tokenizer_config.json (deflated 84%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/adapter_model.safetensors (deflated 8%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/special_tokens_map.json (deflated 70%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/config.json (deflated 50%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/README.md (deflated 66%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/adapter_config.json (deflated 55%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/generation_config.json (deflated 37%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/tokenizer.json (deflated 81%)\n",
            "  adding: DeepSeek-R1-Test-Qwen-COT/pytorch_model.bin (deflated 12%)\n"
          ]
        }
      ]
    }
  ]
}